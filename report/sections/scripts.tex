\section{Script}\label{sec:script}

\subsection{populate.R}\label{sec:script-populate}
\begin{verbatim}
# Read tables
test = read.csv(file="data/test.csv")
train = read.csv(file="data/train.csv")

# DATETIME # Consider only hour
test$datetime = as.POSIXct(test$datetime)
test$datetime = strptime(test$datetime, "%Y-%m-%d %H:%M:%S")
test$datetime <- as.numeric(format(test$datetime, "%H"))
train$datetime = as.POSIXct(train$datetime)
train$datetime = strptime(train$datetime, "%Y-%m-%d %H:%M:%S")
train$datetime <- as.numeric(format(train$datetime, "%H"))

# HOURS # Treat hour of day as a qualitative covariate
test$datetime = factor(test$datetime)
train$datetime = factor(train$datetime)

# SEASONS # Treat as qualitative covariate
# summer
train$season.summer = rep(0,length(train$season))
train$season.summer[train$season == 2] = 1
test$season.summer = rep(0,length(test$season))
test$season.summer[test$season == 2] = 1
# fall
train$season.fall = rep(0,length(train$season))
train$season.fall[train$season == 3] = 1
test$season.fall = rep(0,length(test$season))
test$season.fall[test$season == 3] = 1
# bring all to 0 except spring
test$season[test$season == 2] = 0
test$season[test$season == 3] = 0
test$season[test$season == 4] = 0
train$season[train$season == 2] = 0
train$season[train$season == 3] = 0
train$season[train$season == 4] = 0

# COSTANTS # Some constants
columns = colnames(test)
MAX_P_DEGREE = 30
F1 = as.formula(paste("log(train$count)~",paste(names(train[-c(10:12)]), collapse="+")))

# LIFT-ROC # Import lift-roc script
source("scripts/lift-roc1.R")

\end{verbatim}

\subsection{linearModel.R}\label{sec:script-linear-model}

\begin{verbatim}
if(!is.factor(train$datetime))
  train$datetime = factor(train$datetime)
r2 = rep(0,length(columns)); fstatistic = rep(0,length(columns))

for(i in 1:length(columns)) {
  print(columns[i])
  myCol = train[columns[i]]
  train.lm = lm(log(train$count)~., data = myCol)
  r2[i] = summary(train.lm)$r.squared
  fstatistic[i] = summary(train.lm)$fstatistic[1]
  print(paste(columns[i], "has R-squared:", r2[i], " and F-statistic: ", fstatistic[i]))
}

best = 1
for(i in 2:length(columns)) {
  if(fstatistic[i] > fstatistic[best])
    best = i
}

print(paste(columns[best], "has best F-statistic:", fstatistic[best]))
print(paste(columns[best], "will be used to calculate the linear model w/ 1 variable"))

train.lm = lm(log(train$count) ~ ., data = train[columns[best]])

already_present = c(columns[best])
source("scripts/linear_model_forward_steps.R")
print(already_present)

#clean
rm(i)
rm(best)
rm(r2)
rm(fstatistic)
rm(already_present)
rm(myCol)
\end{verbatim}

\subsection{linear\_model\_forward\_steps.R}
\label{sec:script-linear-model-fwd-steps}

\begin{verbatim}
best_aic = AIC(train.lm)
#already_present = c(already_present,42)

for(i in 1:length(columns)) {
  if(columns[i] %in% already_present)
    next
  myFrame = train[,c(already_present,columns[i])]
  train.lm2 = lm(log(train$count) ~ .,data = myFrame)
  
  aicCur = AIC(train.lm2)
  #myAnova = anova(train.lm,train.lm2)
  #print(columns[i])
  #str(myAnova$`Pr(>F)`)
  if(aicCur < best_aic) {
    best_aic = aicCur
    best = i
  }
}

if(best_aic < AIC(train.lm)) {
  already_present = c(already_present,columns[best])
  myFrame = train[,c(already_present)]
  train.lm = lm(log(train$count) ~ .,data = myFrame)
  print(paste(columns[best], "column has been added to our linear model"))
  source("scripts/linear_model_forward_steps.R")
}

#clean
if(exists("myFrame"))
  rm(myFrame)
if(exists("myAnova"))
  rm(myAnova)
if(exists("best_aic"))
  rm(best_aic)
if(exists("aicCur"))
  rm(aicCur)
\end{verbatim}

\subsection{KalmanFilter.R}\label{sec:script-kalman}
\begin{verbatim}
indexes = sample(1:NROW(train),length(train$count)-10)

Aprimo = train[indexes,columns]
y = log(train[indexes,"count"])
X = model.matrix(~., data=Aprimo)

V = diag(1,ncol(X),ncol(X))
beta = matrix( c( mean(y[1]), rep(0, ncol(X) - 1) ), ncol(X), 1 )

beta.storia = matrix(NA, nrow(X), ncol(X))
beta.storia[1,] = beta

for(i in 1 : NROW(X)) {
  H = 1 / (1 + t(X[i,]) %*% V %*% X[i,] )
  beta = beta + H[1] * (V %*% X[i,] %*% (y[i] - t(X[i,] %*% beta)) )
  V = V - H[1] * (V %*% X[i,] %*% t(X[i,]) %*% V )
  beta.storia[i,] = beta
}

beta = matrix(beta)
rownames(beta) = colnames(X)

X11()
par(mfrow = c(3,1))
plot(beta.storia[,1], type="l")
plot(beta.storia[,2], type="l")
plot(beta.storia[,3], type="l")

rm(Aprimo)
rm(H)
rm(V)
rm(X)
rm(i)
rm(y)
rm(indexes)
rm(beta.storia)
\end{verbatim}

\subsection{MySM\_temp.R}\label{sec:mySM-temp}
\begin{verbatim}
# LOCAL REGRESSION #
# Load 'sm' library
library(sm)

# Set 'temp' as covariate and 'count' as response
x = train$temp
y = log(train$count)

# Choose h interval
#interv = 10:100
#divisions= 10
#interv = 100:200
#divisions= 100
#interv = 1700:2000
#divisions= 1000
interv = 18500:19000
divisions= 10000

# Draw plot in which 'count' is function of 'temp'
plot(x,y,col=3,pch=3)

# Run several times local regression and store prediction errors
val = matrix(NA, length(interv),1)
for (i in interv) {
  inc = i / divisions
  sm1 = sm.regression(x,y,h=inc,add=TRUE,ngrid=120,col=2)
  val[i-min(interv)] = sum((y[1:120]-sm1$estimate)^2)
}

# Draw errors as a function of h values
plot((interv)/divisions,val)

# Find best h
val = val[1:(length(val)-1)]
i = which(val == min(val))
optimal_h = interv[i]/divisions

# Clean workspace (but keep best h)
rm(divisions)
rm(i)
rm(inc)
rm(interv)
rm(sm1)
rm(x)
rm(val)
rm(y)
\end{verbatim}

\subsection{drawOptimalSM\_temp.R}\label{sec:drawSM-temp}
\begin{verbatim}
# DRAW LOCAL REGRESSION #
# Set 'temp' as covariate and 'count' as response
x = train$temp
y = log(train$count)

# Draw local regression with optimal h
plot(x,y,col=3,pch=3)
sm1 = sm.regression(x,y,h=optimal_h,add=TRUE,ngrid=120,col=2)

# Clean workspace
rm(x)
rm(y)
\end{verbatim}

\subsection{loess\_temp.R}\label{sec:loess-temp}
\begin{verbatim}
# LOESS #
# Get indexes for training & test set
indexes = sample(1:length(train$count), size=length(train$count)/2)
indexes.v = sample(setdiff(1:length(train$count), indexes))
column = "temp"

# Get actual training & test set
x = train[column]
x.v = x[indexes.v,column]
x = x[indexes,column]
y = log(train$count)
y.v = y[indexes.v]
y = y[indexes]

# Find best span by X-validation
nummin = 1
num = 10
val = matrix(NA,num,2)
for(i in nummin:num) {
  k=i/10
  val[i,1] = k
  lo1 = loess(y~x,span=k)
  val[i,2] = sum((y.v-predict(lo1))^2)
}

# Show error as a function of span
plot(val[nummin:num,1],val[nummin:num,2],xlab="Span",ylab="Errore")
print(val[nummin:num,])
optimal_span = min(val[,2])
optimal_span = val[,1][val[,2] == optimal_span]
print(paste("Optimal span is",optimal_span))
cat("Premere <Enter>"); readline()

# Draw loess with best span
lo1 <- loess.smooth(x,y,span=optimal_span)
plot(x,y,pch=2,col=2,main="loess")
points(x,y.v,pch=3,col=3)
lines(lo1)

# Clean workspace
rm(i)
rm(indexes)
rm(indexes.v)
rm(h)
rm(lo1)
rm(num)
rm(nummin)
rm(val)
rm(x)
rm(x.v)
rm(y)
rm(y.v)
\end{verbatim}

\subsection{regression-splines-temp.R}\label{sec:regression-splines-temp}
\begin{verbatim}
# REGRESSION SPLINES #
# Load 'splines' library
library(splines)

# Get indexes for training & test set
indexes = sample(1:length(train$count), size=length(train$count)/2)
indexes.v = sample(setdiff(1:length(train$count), indexes))
str(indexes)
str(indexes.v)

# Get actual training & test set
x = train$temp
str(x)
x.v = x[indexes.v]
x = x[indexes]
y = log(train$count)
y.v = y[indexes.v]
y = y[indexes]

# Plot training (red) & test (green) set
plot(x,y,pch=2,col=2,main="splines di regressione")
points(x,y.v,pch=3,col=3)

# Draw regression spline of arbitrary length
xi = seq(min(x),max(x),length=13)
xx = sort(c(x,xi[2:(length(xi)-1)]))
m1 = lm(y~bs(x,knots=xi[2:(length(xi)-1)],degree=3))
fit1 = predict(m1,data.frame(x=xx))
lines(xx,fit1)
# Draw splines intervals
for(i in 1:length(xi)) {
  abline(v=xi[i], lty=3)
}

# Find best number of knots by X-validation
nummin = 3
num = 25
val = matrix(NA,num,2)
for (i in nummin:num){
  h = i
  val[i,1] = h
  xi= seq(min(x),max(x),length=i+2)
  xx = sort(c(x,xi[2:(length(xi)-1)]))
  m1 = lm(y~bs(x,knots=xi[2:(length(xi)-1)],degree=3))
  
  val[i,2] = sum((y.v-predict(m1))^2)
}

# Show error as a function of number of knots and find best #knots
plot(val[nummin:num,1], val[nummin:num,2],xlab="Numero di nodi",ylab="Errore")
print(val[nummin:num,])
val = na.omit(val)
bestVal = min(val[,2])
pos = val[,1][val[,2]==bestVal]
print(paste("Wow",bestVal,"in position",pos))
cat("Premere <Enter>"); readline()

# Draw spline with best number of knots
plot(x,y,pch=2,col=2,main="splines di regressione",xlab="temp",ylab="count")
points(x,y.v,pch=3,col=3)
xi<-seq(min(x),max(x),length=pos)
xx<-sort(c(x,xi[2:(length(xi)-1)]))
m1<-lm(y~bs(x,knots=xi[2:(length(xi)-1)],degree=3))
fit1<-predict(m1,data.frame(x=xx))
lines(xx,fit1,col=4)
# Draw splines intervals
for(i in 1:length(xi)) {
  abline(v=xi[i], lty=3)
}

# Clean workspace
rm(bestVal)
rm(fit1)
rm(h)
rm(i)
rm(indexes)
rm(indexes.v)
rm(m1)
rm(num)
rm(nummin)
rm(pos)
rm(val)
rm(x)
rm(x.v)
rm(xi)
rm(xx)
rm(y)
rm(y.v)
\end{verbatim}

\subsection{smoothing-splines-temp.R}\label{sec:smoothing-splines-temp}
\begin{verbatim}
# SMOOTHING SPLINES #
# Load 'sm' library
library(sm)

# Get indexes for training & test set
indexes = sample(1:length(train$count), size=length(train$count)/2)
indexes.v = sample(setdiff(1:length(train$count), indexes))
column = "temp"

# Get actual training & test set
if(column == "datetime")
  x = as.numeric(train[column])
x = train[column]
str(x)
x.v = x[indexes.v,column]
x = x[indexes,column]
y = log(train$count)
y.v = y[indexes.v]
y = y[indexes]

# Plot training (red) & test (green) set
plot(x,y,pch=2,col=2,main="Splines di lisciamento")
points(x,y.v,pch=3,col=3)

# Create a dataset in which means of response variable respective to
# covariates are stored
myFrame = data.frame(x,y)
levs = unique(x)
med = rep(0,length(levs))
i = 0
for(l in levs) {
  i = i + 1
  cnt = length(with(myFrame, y[x==l]))
  med[i] = sum(with(myFrame, y[x==l])) / cnt
}

# Save old x & y
x_old = x
y_old = y
x = levs;
y = med;

# Same thing for test set
myFrame = data.frame(x.v,y.v)
levs = unique(x.v)
med = rep(0,length(levs))
i = 0
for(l in levs) {
  i = i + 1
  cnt = length(with(myFrame, y.v[x.v==l]))
  med[i] = sum(with(myFrame, y.v[x.v==l])) / cnt
}

# Same as above
x.v = levs
y.v = med

# Draw a first spline, without specifying lambda
s1 <- smooth.spline(x,y)
lines(s1)
cat("premere <Enter>"); readline()

# Find best lambda by X-validation
nummin <- 1
num <- 100
val <- matrix(NA,num,2)
for (i in nummin:num){
  h=i/100
  val[i,1]<-h
  s1 <- smooth.spline(x,y,spar=h)
  val[i,2]<-sum((y.v-predict(s1)$y)^2)
}

# Plot error as a function of lambda and save best lambda
plot(val[nummin:num,1], val[nummin:num,2],xlab="Lambda",ylab="Errore")
val[nummin:num,]
val = na.omit(val)
bestVal = min(val[,2])
pos = val[,1][val[,2]==bestVal]
print(paste("Best:",bestVal,"in position",pos))
cat("premere <Enter>"); readline()

# Plot smoothing splines with best lambda
plot(x_old,y_old,pch=2,col=2,main="Splines di lisciamento",xlab=column,ylab="Bike sharing request")
points(x.v,y.v,pch=3,col=3)
s1 <- smooth.spline(x,y,spar=pos)
lines(s1)

# Clean workspace
rm(bestVal)
rm(cnt)
rm(h)
rm(i)
rm(indexes)
rm(indexes.v)
rm(l)
rm(levs)
rm(med)
rm(myFrame)
rm(num)
rm(nummin)
rm(pos)
rm(s1)
rm(val)
rm(x)
rm(x_old)
rm(x.v)
rm(y)
rm(y_old)
rm(y.v)
\end{verbatim}

\subsection{mars.R}\label{sec:mars-script}
\begin{verbatim}
# MARS #
# Load library 'earth'
library(earth)

# Store formula that will be used to compute the MARS model
Fmars = as.formula(paste("y~",paste(names(train[-c(10:12)]), collapse="+")))

# Get indexes for training & test set
indexes = sample(1:length(train$count), size=length(train$count)/2)
indexes.v = sample(setdiff(1:length(train$count), indexes))

# Get actual training & test set
x = train[columns]
y = log(train$count)
x.v = x[indexes.v,]
x = x[indexes,]
y.v = y[indexes.v]
y = y[indexes]

# Find MARS model and plot relative graphs
mars = earth(Fmars,data=x)
plot(mars)
cat("press <Enter>"); readline()
evimp(mars)
plot(evimp(mars))
cat("press <Enter>"); readline()

# Lift-roc
p6 = predict(mars,x.v)
a6 = lift.roc(p6, y.v, type="crude")

# Clean workspace
rm(a6)
rm(p6)
rm(indexes)
rm(indexes.v)
rm(x)
rm(x.v)
rm(y)
rm(y.v)
\end{verbatim}

\subsection{gam.R}\label{sec:gam-script}
\begin{verbatim}
# GAM #
# Load 'gam' library
library(gam)

# Get indexes for training & test set
indexes = sample(1:length(train$count), size=length(train$count)/2)
indexes.v = sample(setdiff(1:length(train$count), indexes))
Fgam = as.formula(paste("y~",paste(names(train[-c(10:12)]), collapse="+")))

# Get actual training & test set
x = train[columns]
x$datetime = as.numeric(x$datetime)
y = train$count
x.v = x[indexes.v,]
x = x[indexes,]
y = log(train$count)
y.v = y[indexes.v]
y = y[indexes]

# Request (and plot) GAM model
gam1 <-  gam(Fgam, family=gaussian, data=x)
summary(gam1)
plot(gam1, ask=T)

# Predict response variable on test set and draw lift-roc plots
p8 <- predict(gam1,newdata=x.v,type="response")
a8<- lift.roc(p8, y.v, type="crude")

# Clean workspace
rm(a8)
rm(Fgam)
rm(indexes)
rm(indexes.v)
rm(p8)
rm(x)
rm(x.v)
rm(y)
rm(y.v)
\end{verbatim}

\subsection{ppr.R}\label{sec:ppr-script}
\begin{verbatim}
# PROJECTION PURSUIT REGRESSION #
# Get indexes for training & test set
indexes = sample(1:length(train$count), size=length(train$count)/2)
indexes.v = sample(setdiff(1:length(train$count), indexes))

# Get actual training & test set
x = train[columns]
x$datetime = as.numeric(x$datetime)
y = train$count
x.v = x[indexes.v,]
x = x[indexes,]
y = log(train$count)
y.v = y[indexes.v]
y = y[indexes]

# Set upper bound for iterations on PPR
UB = 20
errors = rep(0,UB)

# Iterate by changing # of projections value
for(k in 1:UB) {
  ppr1 = ppr(y ~ ., data=x, nterms = k)
  mppr1 = pmax(predict(ppr1,newdata=x.v), .5)
  pPPR = predict(ppr1,newdata=x.v,type="response")
  errors[k] = sum((y.v-pPPR)^2)
  print(paste("Done PPR with k = ", k, "-- error: ", errors[k]))
}

# Find and plot best k value
bestK = which.min(errors)
print(paste("Index ", bestK, " has best error: ",errors[bestK]))
plot(1:UB, errors, xlab = "k", ylab="Errore")
cat("press <Enter>"); readline()

# Get best PPR
ppr1 = ppr(y ~ ., data=x, nterms = bestK)
pPPR = predict(ppr1,newdata=x.v,type="response")

# Plot lift-roc for best PPR
aPPR = lift.roc(pPPR, y.v, type="crude")

# Clean workspace
rm(aPPR)
rm(bestK)
rm(errors)
rm(indexes)
rm(indexes.v)
rm(k)
rm(mppr1)
rm(pPPR)
#rm(ppr1)
rm(UB)
rm(x)
rm(x.v)
rm(y)
rm(y.v)
\end{verbatim}

\subsection{neural-network.R}\label{sec:nnet-script}
\begin{verbatim}
# RETI NEURALI #
# Load 'nnet' library
library(nnet)

# Store formula that will be used to compute the neural network
Fnet = as.formula(paste("log(count)~",paste(names(train[-c(10:12)]), collapse="+")))

# Get indexes for training & test set
indexes = sample(1:length(train$count), size=length(train$count)/2 + 1)
indexes.v = sample(setdiff(1:length(train$count), indexes))

# Get actual training & test set
x = train[c(columns,"count")]
x$datetime = as.numeric(x$datetime)
x.v = x[indexes.v,]
x = x[indexes,]

# Try to converge with 100 iterations and get predicted values
n1 = nnet(Fnet, data = x, decay = 0.0016, size = 5, maxit = 100)
pNN = predict(n1, newdata = x.v)

# Plot lift-roc for best PPR
lift.roc(pNN,x$count,type="crude")

# Clean workspace
rm(Fnet)
rm(indexes)
rm(indexes.v)
rm(x)
rm(x.v)
\end{verbatim}

\subsection{tree.R}\label{sec:tree-script}
\begin{verbatim}
# ALBERI DI REGRESSIONE #
# Load 'tree' library
library(tree)

# Store formula that will be used to find the regression tree
Ftree = as.formula(paste("log(count)~",paste(names(train[-c(10:12)]), collapse="+")))

# Get indexes for training & test set
indexes = sample(1:length(train$count), size=length(train$count)/2 + 1)
indexes.v = sample(setdiff(1:length(train$count), indexes))

# Get actual training & test set
x = train[c(columns,"count")]
x$datetime = as.numeric(x$datetime)
y = train$count
x.v = x[indexes.v,]
x = x[indexes,]
parte1 = sample(1:NROW(x),length(indexes)/2)
parte2 = setdiff(1:NROW(x),parte1)

# Create complete tree w/ deviance threshold = .02
t1 = tree(Ftree, data= x[parte1,], control = tree.control(nobs=length(parte1),minsize=2, mindev = 0.02))  # fino alle foglie
plot(t1)
text(t1)
cat("press <Enter>"); readline()

# Fathom the tree in all the possible ways
t2 = prune.tree(t1, newdata = x[parte2,])
plot(t2)
cat("press <Enter>"); readline()

# Find best pruning and then fathom the tree to that size
J = t2$size[t2$dev == min(t2$dev)]
t3 = prune.tree(t1, best=J)
plot(t3)
text(t3)

# Clean workspace
rm(Ftree)
rm(indexes)
rm(indexes.v)
rm(parte1)
rm(parte2)
rm(t1)
rm(t2)
rm(x)
rm(x.v)
rm(y)
\end{verbatim}

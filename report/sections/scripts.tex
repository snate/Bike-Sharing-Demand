\section{Script}\label{sec:script}

\subsection{populate.R}\label{sec:script-populate}
\begin{verbatim}
# Read tables
test = read.csv(file="data/test.csv")
train = read.csv(file="data/train.csv")

# DATETIME # Consider only hour
test$datetime = as.POSIXct(test$datetime)
test$datetime = strptime(test$datetime, "%Y-%m-%d %H:%M:%S")
test$datetime <- as.numeric(format(test$datetime, "%H"))
train$datetime = as.POSIXct(train$datetime)
train$datetime = strptime(train$datetime, "%Y-%m-%d %H:%M:%S")
train$datetime <- as.numeric(format(train$datetime, "%H"))

# HOURS # Treat hour of day as a qualitative covariate
test$datetime = factor(test$datetime)
train$datetime = factor(train$datetime)

# SEASONS # Treat as qualitative covariate
# summer
train$season.summer = rep(0,length(train$season))
train$season.summer[train$season == 2] = 1
test$season.summer = rep(0,length(test$season))
test$season.summer[test$season == 2] = 1
# fall
train$season.fall = rep(0,length(train$season))
train$season.fall[train$season == 3] = 1
test$season.fall = rep(0,length(test$season))
test$season.fall[test$season == 3] = 1
# bring all to 0 except spring
test$season[test$season == 2] = 0
test$season[test$season == 3] = 0
test$season[test$season == 4] = 0
train$season[train$season == 2] = 0
train$season[train$season == 3] = 0
train$season[train$season == 4] = 0

# COSTANTS # Some constants
columns = colnames(test)
MAX_P_DEGREE = 30
F1 = as.formula(paste("log(train$count)~",paste(names(train[-c(10:12)]), collapse="+")))

# LIFT-ROC # Import lift-roc script
source("scripts/lift-roc1.R")

\end{verbatim}

\subsection{linearModel.R}\label{sec:script-linear-model}

\begin{verbatim}
if(!is.factor(train$datetime))
  train$datetime = factor(train$datetime)
r2 = rep(0,length(columns)); fstatistic = rep(0,length(columns))

for(i in 1:length(columns)) {
  print(columns[i])
  myCol = train[columns[i]]
  train.lm = lm(log(train$count)~., data = myCol)
  r2[i] = summary(train.lm)$r.squared
  fstatistic[i] = summary(train.lm)$fstatistic[1]
  print(paste(columns[i], "has R-squared:", r2[i], " and F-statistic: ", fstatistic[i]))
}

best = 1
for(i in 2:length(columns)) {
  if(fstatistic[i] > fstatistic[best])
    best = i
}

print(paste(columns[best], "has best F-statistic:", fstatistic[best]))
print(paste(columns[best], "will be used to calculate the linear model w/ 1 variable"))

train.lm = lm(log(train$count) ~ ., data = train[columns[best]])

already_present = c(columns[best])
source("scripts/linear_model_forward_steps.R")
print(already_present)

#clean
rm(i)
rm(best)
rm(r2)
rm(fstatistic)
rm(already_present)
rm(myCol)
\end{verbatim}

\subsection{linear\_model\_forward\_steps.R}
\label{sec:script-linear-model-fwd-steps}

\begin{verbatim}
best_aic = AIC(train.lm)
#already_present = c(already_present,42)

for(i in 1:length(columns)) {
  if(columns[i] %in% already_present)
    next
  myFrame = train[,c(already_present,columns[i])]
  train.lm2 = lm(log(train$count) ~ .,data = myFrame)
  
  aicCur = AIC(train.lm2)
  #myAnova = anova(train.lm,train.lm2)
  #print(columns[i])
  #str(myAnova$`Pr(>F)`)
  if(aicCur < best_aic) {
    best_aic = aicCur
    best = i
  }
}

if(best_aic < AIC(train.lm)) {
  already_present = c(already_present,columns[best])
  myFrame = train[,c(already_present)]
  train.lm = lm(log(train$count) ~ .,data = myFrame)
  print(paste(columns[best], "column has been added to our linear model"))
  source("scripts/linear_model_forward_steps.R")
}

#clean
if(exists("myFrame"))
  rm(myFrame)
if(exists("myAnova"))
  rm(myAnova)
if(exists("best_aic"))
  rm(best_aic)
if(exists("aicCur"))
  rm(aicCur)
\end{verbatim}

\subsection{KalmanFilter.R}\label{sec:script-kalman}
\begin{verbatim}
indexes = sample(1:NROW(train),length(train$count)-10)

Aprimo = train[indexes,columns]
y = log(train[indexes,"count"])
X = model.matrix(~., data=Aprimo)

V = diag(1,ncol(X),ncol(X))
beta = matrix( c( mean(y[1]), rep(0, ncol(X) - 1) ), ncol(X), 1 )

beta.storia = matrix(NA, nrow(X), ncol(X))
beta.storia[1,] = beta

for(i in 1 : NROW(X)) {
  H = 1 / (1 + t(X[i,]) %*% V %*% X[i,] )
  beta = beta + H[1] * (V %*% X[i,] %*% (y[i] - t(X[i,] %*% beta)) )
  V = V - H[1] * (V %*% X[i,] %*% t(X[i,]) %*% V )
  beta.storia[i,] = beta
}

beta = matrix(beta)
rownames(beta) = colnames(X)

X11()
par(mfrow = c(3,1))
plot(beta.storia[,1], type="l")
plot(beta.storia[,2], type="l")
plot(beta.storia[,3], type="l")

rm(Aprimo)
rm(H)
rm(V)
rm(X)
rm(i)
rm(y)
rm(indexes)
rm(beta.storia)
\end{verbatim}

\subsection{MySM\_temp.R}\label{sec:mySM-temp}
\begin{verbatim}
# LOCAL REGRESSION #
# Load 'sm' library
library(sm)

# Set 'temp' as covariate and 'count' as response
x = train$temp
y = log(train$count)

# Choose h interval
#interv = 10:100
#divisions= 10
#interv = 100:200
#divisions= 100
#interv = 1700:2000
#divisions= 1000
interv = 18500:19000
divisions= 10000

# Draw plot in which 'count' is function of 'temp'
plot(x,y,col=3,pch=3)

# Run several times local regression and store prediction errors
val = matrix(NA, length(interv),1)
for (i in interv) {
  inc = i / divisions
  sm1 = sm.regression(x,y,h=inc,add=TRUE,ngrid=120,col=2)
  val[i-min(interv)] = sum((y[1:120]-sm1$estimate)^2)
}

# Draw errors as a function of h values
plot((interv)/divisions,val)

# Find best h
val = val[1:(length(val)-1)]
i = which(val == min(val))
optimal_h = interv[i]/divisions

# Clean workspace (but keep best h)
rm(divisions)
rm(i)
rm(inc)
rm(interv)
rm(sm1)
rm(x)
rm(val)
rm(y)
\end{verbatim}

\subsection{drawOptimalSM\_temp.R}\label{sec:drawSM-temp}
\begin{verbatim}
# DRAW LOCAL REGRESSION #
# Set 'temp' as covariate and 'count' as response
x = train$temp
y = log(train$count)

# Draw local regression with optimal h
plot(x,y,col=3,pch=3)
sm1 = sm.regression(x,y,h=optimal_h,add=TRUE,ngrid=120,col=2)

# Clean workspace
rm(x)
rm(y)
\end{verbatim}

\subsection{loess\_temp.R}\label{sec:loess-temp}
\begin{verbatim}
# LOESS #
# Get indexes for training & test set
indexes = sample(1:length(train$count), size=length(train$count)/2)
indexes.v = sample(setdiff(1:length(train$count), indexes))
column = "temp"

# Get actual training & test set
x = train[column]
x.v = x[indexes.v,column]
x = x[indexes,column]
y = log(train$count)
y.v = y[indexes.v]
y = y[indexes]

# Find best span by cross validation
nummin = 1
num = 10
val = matrix(NA,num,2)
for(i in nummin:num) {
  k=i/10
  val[i,1] = k
  lo1 = loess(y~x,span=k)
  val[i,2] = sum((y.v-predict(lo1))^2)
}

# Show error as a function of span
plot(val[nummin:num,1],val[nummin:num,2],xlab="Span",ylab="Errore")
print(val[nummin:num,])
optimal_span = min(val[,2])
optimal_span = val[,1][val[,2] == optimal_span]
print(paste("Optimal span is",optimal_span))
cat("premere <Enter>"); readline()

# Draw loess with best span
lo1 <- loess.smooth(x,y,span=optimal_span)
plot(x,y,pch=2,col=2,main="loess")
points(x,y.v,pch=3,col=3)
lines(lo1)

# Clean workspace
rm(i)
rm(indexes)
rm(indexes.v)
rm(h)
rm(lo1)
rm(num)
rm(nummin)
rm(val)
rm(x)
rm(x.v)
rm(y)
rm(y.v)
\end{verbatim}
